# ğŸ—ï¸ Hybrid Crawler Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         REACT DASHBOARD                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ New Project  â”‚  â”‚ Start Crawl  â”‚  â”‚ View Stats  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                 â”‚                  â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â–¼                 â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SUPABASE DATABASE (The Brain)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   projects   â”‚  â”‚crawl_sessionsâ”‚  â”‚crawl_queue  â”‚               â”‚
â”‚  â”‚  (websites)  â”‚  â”‚  (tracking)  â”‚  â”‚  (pending)  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    pages     â”‚  â”‚internal_linksâ”‚  â”‚opportunitiesâ”‚               â”‚
â”‚  â”‚  (results)   â”‚  â”‚ (graph data) â”‚  â”‚(suggestions)â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                â”‚
           â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TYPESCRIPT CRAWLER    â”‚      â”‚    PYTHON WORKER         â”‚
â”‚  (Edge Function)       â”‚      â”‚    (External Process)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… 10-1000 pages       â”‚      â”‚ âœ… 1000-100,000 pages    â”‚
â”‚ âœ… Click & forget      â”‚      â”‚ âœ… No timeout limits     â”‚
â”‚ âœ… Zero setup          â”‚      â”‚ âœ… Runs for days         â”‚
â”‚ âš ï¸  60s per invocation â”‚      â”‚ âš ï¸  Requires Python      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### Starting a Crawl (User Action)

```
User clicks "Start Crawl"
    â†“
Dashboard creates:
    1. crawl_session record
    2. Initial crawl_queue entry (start URL)
    â†“
Database now has:
    - session_id: abc-123
    - crawl_queue: [{ url: "https://example.com", status: "pending" }]
```

### TypeScript Crawler Flow

```
Edge Function invoked
    â†“
1. SELECT * FROM crawl_queue WHERE status='pending' LIMIT 10
    â†“
2. For each URL:
    - Fetch HTML
    - Parse content
    - Extract links
    - Save to pages table
    - Add new links to crawl_queue
    - Mark as 'completed'
    â†“
3. Self-invoke if queue not empty
    â†“
Repeat until queue empty or max_pages reached
```

### Python Worker Flow

```
python worker.py starts
    â†“
1. SELECT * FROM crawl_queue WHERE status='pending' LIMIT 5
    â†“
2. For each URL:
    - Update status to 'processing'
    - Fetch HTML
    - Parse content
    - Extract links
    - Save to pages table
    - Add new links to crawl_queue
    - Mark as 'completed'
    â†“
3. Loop forever (no timeout!)
    â†“
Stops when queue empty or Ctrl+C
```

## Crash Recovery

### What Happens When It Crashes

```
Crawler running...
Page 1: âœ… Saved to DB
Page 2: âœ… Saved to DB
Page 3: âœ… Saved to DB
Page 4: âš¡ CRASH!
```

**TypeScript:**
```
crawl_queue table still has:
- Page 4: status = 'pending' âŒ (was processing)
- Page 5-1000: status = 'pending' âœ…

Just restart crawl â†’ picks up from Page 4
```

**Python:**
```
crawl_queue table still has:
- Page 4: status = 'processing' âš ï¸
- Page 5-1000: status = 'pending' âœ…

Run: python worker.py
    â†“
Skips 'processing' (will timeout after 1 hour)
Starts with Page 5 âœ…
```

## Real-Time Updates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Worker  â”‚         â”‚    Database     â”‚         â”‚   Dashboard     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚                           â”‚
         â”‚ 1. Crawl page            â”‚                           â”‚
         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚                           â”‚
         â”‚                           â”‚                           â”‚
         â”‚ 2. Save to pages table   â”‚                           â”‚
         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚                           â”‚
         â”‚                           â”‚                           â”‚
         â”‚                           â”‚ 3. Dashboard polls        â”‚
         â”‚                           â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
         â”‚                           â”‚                           â”‚
         â”‚                           â”‚ 4. Return new pages       â”‚
         â”‚                           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
         â”‚                           â”‚                           â”‚
         â”‚                           â”‚                  âœ¨ User sees update!
```

## Parallel Processing

### Multiple Python Workers

```
Worker 1                    Database                    Worker 2
   â†“                           â†“                           â†“
GET pending URL 1          crawl_queue              GET pending URL 2
   â†“                           â†“                           â†“
Process URL 1             [URL 1: processing]        Process URL 2
   â†“                      [URL 2: processing]             â†“
Save results              [URL 3: pending]           Save results
   â†“                      [URL 4: pending]                â†“
COMPLETE                  [URL 1: completed]         COMPLETE
   â†“                      [URL 2: completed]              â†“
GET URL 3                 [URL 3: processing] â†â”€â”€â”€â”€â”€ GET URL 4
   â†“                      [URL 4: processing]             â†“
```

**No conflicts!** Database ensures each URL is only fetched once via:
- `UNIQUE(session_id, normalized_url)` constraint
- Atomic status updates

## Database Tables Explained

### crawl_queue (The To-Do List)

| Column | Purpose | Example |
|--------|---------|---------|
| `session_id` | Which crawl this belongs to | `abc-123` |
| `url` | What to crawl | `https://example.com/page` |
| `normalized_url` | Deduplicated version | `https://example.com/page` |
| `depth` | How many clicks from homepage | `2` |
| `parent_url` | Where we found this link | `https://example.com` |
| `status` | Current state | `pending`, `processing`, `completed`, `failed` |

### pages (The Results)

| Column | Purpose | Example |
|--------|---------|---------|
| `project_id` | Which website | `proj-456` |
| `url` | Page URL | `https://example.com/about` |
| `title` | Page title | `About Us` |
| `content` | Page text | `We are a company...` |
| `word_count` | Content length | `350` |
| `internal_links_count` | Links found | `25` |
| `page_type` | Classified type | `informational` |
| `keywords` | Extracted keywords | `["company", "team"]` |
| `link_equity_score` | SEO metric | `75.5` |

## Choosing the Right Crawler

```
                    TypeScript          Python
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€
Pages needed        < 1000              > 1000
Setup time          0 minutes           5 minutes
Infrastructure      None                Local/VPS
Speed              20-30 pages/min     60-100 pages/min
Timeout limit       Yes (60s)           No
Dashboard control   âœ… Full             âœ… Full
Resume after crash  âœ… Yes              âœ… Yes
Multiple workers    âŒ No               âœ… Yes
Cost                Supabase only       Supabase + compute
```

## Decision Tree

```
Need to crawl a website?
    â†“
    â”œâ”€ Less than 1000 pages?
    â”‚   â†“
    â”‚   YES â†’ Use TypeScript crawler
    â”‚         (Click "Start Crawl" in dashboard)
    â”‚
    â””â”€ More than 1000 pages?
        â†“
        YES â†’ Use Python worker
              1. Click "Start Crawl" in dashboard
              2. Copy session ID
              3. Run: python worker.py
```

## Hybrid in Action

### Scenario: Crawling 5000 page site

**Step 1: Start in Dashboard**
```
User: Create project â†’ Click "Start Crawl"
DB: session_id = abc-123
    crawl_queue = [homepage]
```

**Step 2: TypeScript tries (optional)**
```
TypeScript: Processes 100 pages
            60s timeout hits
            Stops (100 pages saved âœ…)
DB: pages = 100
    crawl_queue = 4900 pending URLs
```

**Step 3: Python takes over**
```
Terminal: python worker.py
Python: Picks up remaining 4900 URLs
        Runs for 2 hours (no timeout!)
        Completes all 5000 pages âœ…
```

**Result:** Dashboard shows all 5000 pages! ğŸ‰

---

## Summary

**The Brain:** Database (Supabase)
**The Muscle:** TypeScript OR Python (your choice)
**The Interface:** React Dashboard (always)

Both crawlers speak the same language (database tables), so they work together seamlessly!
