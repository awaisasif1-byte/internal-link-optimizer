# ğŸ•·ï¸ Local Crawler Setup

Your crawler is now ready to run **on your local PC** with no timeout limits!

## ğŸš€ Quick Start Guide

### Step 1: Download and Install

1. **Download this project** to your local machine
2. **Open terminal/command prompt** in the project folder
3. **Install dependencies:**
   ```bash
   npm install
   ```

### Step 2: Configure Environment Variables

1. **Copy the example file:**
   ```bash
   cp .env.example .env
   ```

2. **Edit `.env` and add your Supabase credentials:**
   - Go to your Supabase project dashboard
   - Click **Settings** (gear icon) â†’ **API**
   - Copy "**Project URL**" â†’ paste as `SUPABASE_URL`
   - Copy "**service_role**" secret â†’ paste as `SUPABASE_SERVICE_ROLE_KEY`

### Step 3: Start a Crawl in the Web App

1. Open your web application
2. Create a new project
3. Click "**Start Crawl**"
4. **Copy the session ID** from the browser URL or console logs

### Step 4: Run the Local Crawler

```bash
npm run crawl -- --session YOUR_SESSION_ID_HERE
```

**Example:**
```bash
npm run crawl -- --session abc123-def456-ghi789
```

---

## ğŸ“Š What You'll See

```
========================================
ğŸ•·ï¸  LOCAL CRAWLER STARTED
========================================
ğŸ“‹ Session ID: abc123-def456
ğŸŒ Supabase URL: https://your-project.supabase.co
========================================

ğŸŒ [1/100] Fetching: https://example.com/
  âœ… Title: Example Domain
  ğŸ“Š 500 words, 25 internal links found
  â• Added 25 new URLs to queue

ğŸŒ [2/100] Fetching: https://example.com/about
  âœ… Title: About Us
  ğŸ“Š 750 words, 18 internal links found
  â• Added 10 new URLs to queue

...

========================================
âœ… CRAWL COMPLETED!
========================================
ğŸ“„ Pages Processed: 100
ğŸ”— Links Discovered: 452
âŒ Errors: 2
â±ï¸  Duration: 180s
âš¡ Speed: 0.56 pages/sec
========================================
```

---

## ğŸ¯ Benefits of Local Crawling

âœ… **No timeout limits** - Crawl 1000s of pages  
âœ… **Full control** - See real-time logs in your terminal  
âœ… **Easy debugging** - Error messages appear immediately  
âœ… **Pause/Resume** - Press `Ctrl+C` to stop (status saved)  
âœ… **Fast & Reliable** - Uses your PC's resources

---

## âš ï¸ Troubleshooting

### "Missing environment variables"
Make sure you created the `.env` file with correct Supabase credentials.

### "Session not found"
Start a crawl in the web app first, then copy the session ID.

### "Cannot connect to database"
Double-check your `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY`.

---

## ğŸ›‘ Stopping a Crawl

Press `Ctrl+C` in your terminal to gracefully stop the crawler. The session status will be marked as "stopped" in the database.

---

## ğŸ“ Notes

- The crawler connects directly to your Supabase database
- Crawled data appears in your web dashboard in real-time
- One crawler per session - don't run multiple at once
- Respects a 500ms delay between requests (be nice to servers!)

---

**Happy Crawling!** ğŸš€
